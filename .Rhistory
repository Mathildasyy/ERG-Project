## Set 2 by 2 canvas
par(mfrow = c(2,2), mar = c(2,2,2,2))
# Then, we plot the numeric and non-numeric variables in different ways
feature.graphs <- function(subset.index){
for (i in subset.index){
class <- feature.class[i]
title <- names(train)[i]
if(class == 'character'){
boxplot(SalePrice.log~., data = train[,c(i,81)], main = title)
}else
data <- data.frame(x = train[,i],
y = train$SalePrice.log)
ggplot(data, aes(x,y)) +
ggtitle(title)+
theme(plot.title = element_text(hjust = 0.5))+
geom_point(col='grey60', size = 0.5) +
geom_smooth(method="lm", formula= y~poly(x,3, raw =T), colour = 'hotpink3')}
}
# Before we start automatic generation, we have to manually re-classify the feature type
## Delete the hashtags before you name, and input a vector of the actual types
sy<- c('character','character','integer','integer','character',
'character','character','character','character','character',
'character','character','character','character','character',
'character',"integer","integer","integer","integer")
##hy<- c()
##hsq<- c()
##zqf<- c()
feature.class[1:20] = sy
##feature.class[21:40] = hy
##feature.class[41:60] = hsq
##feature.class[61:79] = zqf
# change the index to the subset
feature.subset <- c(5,6,8,11,12,14,9,36,39,41,52,60,61,68,69,70,
71,74,22,43,45,46,55,27,30,31,35,72,75)
# generate your part of graphs
feature.graphs(feature.subset)
# Zero- and Near Zero-Variance Predictors
dim(train) # dim: 1460   80
nzv <- nearZeroVar(train, saveMetrics = T)
nzv[nzv$nzv,][,]
nzv <- nearZeroVar(train)
train.2 <- train[, -nzv] # exclude the close-to-zero-var features
dim(train.2) # 1460   59
# we deduct 21 variables !!(CAUTION! parameters need to be adjusted)
# import packages
library(ggplot2)
library(graphics)
library(splines)
library(caret)
# read data files
train <- read.csv('train.csv')[,-1]
test <- read.csv('test.csv')
summary(train)
# Now, let's observe the training data
## First, we make a histogram of the sale prices
## It is obvious that the distribution is skewed to right
ggplot(train, aes(x = SalePrice/1000)) + # Measure the house prices in $1,000
geom_histogram(binwidth = 30)
train$SalePrice.log <- log(train$SalePrice/1000+1) # log transformation
ggplot(train, aes(x = SalePrice.log)) +
geom_histogram(binwidth = .1)
#  We roughly abstract the class of each feature
feature.class <- c()
for (i in 1:79){
feature.class <- append(feature.class,class(train[,i]))
}
## To observe qualitative features, we use boxplots to idnetify outliers.
## To observe quantitative features, we draw basis-spline plots.
## Set 2 by 2 canvas
par(mfrow = c(2,2), mar = c(2,2,2,2))
# Then, we plot the numeric and non-numeric variables in different ways
feature.graphs <- function(subset.index){
for (i in subset.index){
class <- feature.class[i]
title <- names(train)[i]
if(class == 'character'){
boxplot(SalePrice.log~., data = train[,c(i,81)], main = title)
}else
data <- data.frame(x = train[,i],
y = train$SalePrice.log)
ggplot(data, aes(x,y)) +
ggtitle(title)+
theme(plot.title = element_text(hjust = 0.5))+
geom_point(col='grey60', size = 0.5) +
geom_smooth(method="lm", formula= y~poly(x,3, raw =T), colour = 'hotpink3')}
}
# Before we start automatic generation, we have to manually re-classify the feature type
## Delete the hashtags before you name, and input a vector of the actual types
sy<- c('character','character','integer','integer','character',
'character','character','character','character','character',
'character','character','character','character','character',
'character',"integer","integer","integer","integer")
##hy<- c()
##hsq<- c()
##zqf<- c()
feature.class[1:20] = sy
##feature.class[21:40] = hy
##feature.class[41:60] = hsq
##feature.class[61:79] = zqf
# change the index to the subset
feature.subset <- c(5,6,8,11,12,14,9,36,39,41,52,60,61,68,69,70,
71,74,22,43,45,46,55,27,30,31,35,72,75)
# generate your part of graphs
feature.graphs(feature.subset)
num.feature <- c()
cat.feature <- c()
for (i in 1: ncol(train.2)-1){
class <- class(train.2[,i])
if (class == 'numeric') {
num.feature <- c(num.feature, i)
}
else{
cat.feature <- c(cat.feature, i)
}
}
cat.feature <- cat.feature[-1]
# Fix skewed features
# Extract the numeric data
col_name <- colnames(train.2[,num.feature])
train.2_num <- train.2[,num.feature]
train.2_num <- data.frame(train.2_num)
library(ggpubr)
library(moments)
install.packages('moments')
# Fix skewed features
# Extract the numeric data
col_name <- colnames(train.2[,num.feature])
train.2_num <- train.2[,num.feature]
train.2_num <- data.frame(train.2_num)
library(ggpubr)
library(moments)
# Compute the skewness of each vairiable
skew <- round(skewness(train.2_num, na.rm = FALSE), 3)
num.feature <- c()
cat.feature <- c()
for (i in 1: ncol(train.2)-1){
class <- class(train.2[,i])
if (class == 'numeric') {
num.feature <- c(num.feature, i)
}
else{
cat.feature <- c(cat.feature, i)
}
}
cat.feature <- cat.feature[-1]
# Fix skewed features
# Extract the numeric data
col_name <- colnames(train.2[,num.feature])
train.2_num <- train.2[,num.feature]
train.2_num <- data.frame(train.2_num)
library(ggpubr)
library(moments)
# Compute the skewness of each vairiable
skew <- round(skewness(train.2_num, na.rm = FALSE), 3)
# Between continuous/numeric variables
# 连续数值之间的相关性分析
#par = 0.9
#descrCor = ifelse(cov(train[,num.feature], method='spearman')> par, 1,0)
#cor.num2num <- sort(colMeans(descrCor), decreasing = T)
train.num <- train.2[,num.feature]
descrCor <-  cor(train.num)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
train.num2 <- train.num[,-highlyCorDescr] # delete one variable
descrCor2 <- cor(train.num)
summary(descrCor2[upper.tri(descrCor2)])
#heatmap(as.matrix(train[,num.feature]),Rowv = NA,Colv = NA,main="Numeric Features")
#plot(cor.num2num, type = 'b', col = 'lightcoral')
# Between categorical variables
library(sjstats)
cramer_ind = c()
for (i in 1:length(cat.feature)){
for (j in 1:length(cat.feature)){
tab = table(train.2[,cat.feature[i]], train.2[,cat.feature[j]])
cramer_ind = append(cramer_ind, cramer(tab))
}
}
cramer_ma = matrix(cramer_ind, ncol = length(cat.feature))
index_c = c()
for (i in 1:length(cat.feature)){
index_c = append(index_c, mean(cramer_ma[,i]))
}
sort_c = sort(index_c, index.return = T, decreasing = T)
remain.cat <- cat.feature[-c(sort_c$ix[1:5])]
train.cat <- train.2[,remain.cat]
knitr::opts_chunk$set(echo = TRUE)
library(mice)
library(caret)
library(RANN)
train <- read.csv('train.csv')[,-1]
test <- read.csv('test.csv')
# 统计各个predictor中NA出现的次数
colSums(is.na(train))
# 对于NA本来就有具体含义的15个predictor来说，不需要把NA当做missing value来做imputation.
# 参考“How I made top 0.3% on a Kaggle competition”这篇文章的处理方法，为了不把NA误认为是missing value，下面把这15个predictor中的"NA"改成"None". 其中，GarageYrBlt这一个指标暂且把它当做numeric处理，所以改成了0. 后续也可以把这个指标改成character、把NA改成None.
train$Alley[is.na(train$Alley)] <- "None"
train$BsmtQual[is.na(train$BsmtQual)] <- "None"
train$BsmtCond[is.na(train$BsmtCond)] <- "None"
train$BsmtExposure[is.na(train$BsmtExposure)] <- "None"
train$BsmtFinType1[is.na(train$BsmtFinType1)] <- "None"
train$BsmtFinType2[is.na(train$BsmtFinType2)] <- "None"
train$FireplaceQu[is.na(train$FireplaceQu)] <- "None"
train$GarageType[is.na(train$GarageType)] <- "None"
train$GarageFinish[is.na(train$GarageFinish)] <- "None"
train$GarageQual[is.na(train$GarageQual)] <- "None"
train$GarageCond[is.na(train$GarageCond)] <- "None"
train$PoolQC[is.na(train$PoolQC)] <- "None"
train$Fence[is.na(train$Fence)] <- "None"
train$MiscFeature[is.na(train$MiscFeature)] <- "None"
train$GarageYrBlt[is.na(train$GarageYrBlt)] <- 0
# 下面对LotFrontage, MasVnrType, MasVnrArea和Electrical这四个真正有缺失的predictor进行处理
# LotFrontage, MasVnrType, MasVnrArea和Electrical的缺失个数分别为259，8，8，1, 占比分别为17.7%, 0.5%, 0.5%和0.07%. 通常缺失值超过5%的predictor可以选择直接舍弃。不过参考“How I made top 0.3% on a Kaggle competition”这篇文章，还是把所有predictor都保留。
# 对于categorical missing value (MasVnrType, Electrical), 选择用mode填补；
# 对于numerical missing value (LotFrontage, MasVnrArea), 选择用KNN imputation.
# apply mode imputation
# first create function to identify the mode of MasVnrType and Electrical
my_mode <- function(x) {
unique_x <- unique(x)
mode <- unique_x[which.max(tabulate(match(x, unique_x)))]
mode
}
my_mode(train$MasVnrType)
my_mode(train$Electrical)
# the mode of MasVnrType is "None", while the mode of Electrical is "SBrkr". Impute.
train$MasVnrType[is.na(train$MasVnrType)] <- "None"
train$Electrical[is.na(train$Electrical)] <- "SBrkr"
# use preProcess function in caret package to perform knn imputation on missing values
train_model <- preProcess(train, "knnImpute", k=38) # set k to equal to the square root of number of variables
train <- predict(train_model, train) # Using this approach will automatically trigger preProcess to center and scale the data, regardless of what is in the method argument.
# Check: 再次统计各个predictor中NA出现的次数
colSums(is.na(train))
# Zero- and Near Zero-Variance Predictors
dim(train) # dim: 1460   80
nzv <- nearZeroVar(train, saveMetrics = T)
nzv[nzv$nzv,][,]
nzv <- nearZeroVar(train)
train.2 <- train[, -nzv] # exclude the close-to-zero-var features
dim(train.2) # 1460   59
# we deduct 21 variables !!(CAUTION! parameters need to be adjusted)
num.feature <- c()
cat.feature <- c()
for (i in 1: ncol(train.2)-1){
class <- class(train.2[,i])
if (class == 'numeric') {
num.feature <- c(num.feature, i)
}
else{
cat.feature <- c(cat.feature, i)
}
}
cat.feature <- cat.feature[-1]
# Fix skewed features
# Extract the numeric data
col_name <- colnames(train.2[,num.feature])
train.2_num <- train.2[,num.feature]
train.2_num <- data.frame(train.2_num)
library(ggpubr)
library(moments)
# Compute the skewness of each vairiable
skew <- round(skewness(train.2_num, na.rm = FALSE), 3)
# Find the variables whose skewness > 0.5 (standard)
(skew <- skew[abs(skew)>0.5])
# Visualization
# Plot the density distribution of each variable
round(skewness(train.2_num$LotFrontage, na.rm = FALSE), 3)
ggplot(train.2_num, aes(LotFrontage)) +
geom_density() +
#scale_x_continuous(limits = c(-3, 2)) +
stat_overlay_normal_density(color = "red", linetype = "dashed")
# Log transformation of the skewed data
train.2_num$LotFrontage <- log10(train.2_num$LotFrontage)
sum(is.na(train.2_num$LotFrontage))
ggplot(train.2_num, aes(LotFrontage)) +
geom_density() +
stat_overlay_normal_density(color = "red", linetype = "dashed")
round(skewness(train.2_num$LotFrontage, na.rm = FALSE), 3)
# ggdensity(train.2_num, x = col_name[1], fill = "lightgray", title = paste(col_name[1], "skewness = ", skew[1]), rug = TRUE) +
#   # scale_x_continuous(limits = c(-lim, lim)) +
#   stat_overlay_normal_density(color = "red", linetype = "dashed")
# # train.2_num$MSSubClass <- log10(train.2_num$MSSubClass)
# ggarrange(den_plot, ncol = 2, nrow = 2)
# lim <- max(c(min(train.2_num[1]), max(train.2_num[1])))
# ggdensity(train.2_num, x = col_name[1], fill = "lightgray", title = col_name[1], rug = TRUE) +
#   #scale_x_continuous(limits = c(-point, point)) +
#   stat_overlay_normal_density(color = "red", linetype = "dashed")
# Between continuous/numeric variables
# 连续数值之间的相关性分析
#par = 0.9
#descrCor = ifelse(cov(train[,num.feature], method='spearman')> par, 1,0)
#cor.num2num <- sort(colMeans(descrCor), decreasing = T)
train.num <- train.2[,num.feature]
descrCor <-  cor(train.num)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
train.num2 <- train.num[,-highlyCorDescr] # delete one variable
descrCor2 <- cor(train.num)
summary(descrCor2[upper.tri(descrCor2)])
#heatmap(as.matrix(train[,num.feature]),Rowv = NA,Colv = NA,main="Numeric Features")
#plot(cor.num2num, type = 'b', col = 'lightcoral')
# Between categorical variables
library(sjstats)
cramer_ind = c()
for (i in 1:length(cat.feature)){
for (j in 1:length(cat.feature)){
tab = table(train.2[,cat.feature[i]], train.2[,cat.feature[j]])
cramer_ind = append(cramer_ind, cramer(tab))
}
}
cramer_ma = matrix(cramer_ind, ncol = length(cat.feature))
index_c = c()
for (i in 1:length(cat.feature)){
index_c = append(index_c, mean(cramer_ma[,i]))
}
sort_c = sort(index_c, index.return = T, decreasing = T)
remain.cat <- cat.feature[-c(sort_c$ix[1:5])]
train.cat <- train.2[,remain.cat]
# Excluded variables
names.full <- names(train)[-length(names(train))]
names <- c(names(train.cat),names(train.num2))
log <- names.full %in% names
ex.feature <-c()
for (i in 1:length(log)){
if (log[i] == F) {
ex.feature <- c(ex.feature, names(train)[i])
}
}
ex.feature # These 29 variables are tentatively dropped.
# Between continuous and categorical variables
# 连续数值和类变量之间的相关性分析
# 使用Logistic Regression 通过检验error rate来进行类相关性分析
# 鉴于数值变量组和类变量组的数据都比较多，我们先利用组内删选变量的方式
# 然后再类间通过上述方法进行筛选
#select.cat <- train.cat
#select.num <- train.num2
View(train.cat)
View(sort_c)
View(train.2)
View(train)
knitr::opts_chunk$set(echo = TRUE)
source('~/Documents/Github/ERGProject/preprocessing/preprocess.R')
# Import preparatoins:
library(mice)
library(caret)
library(RANN)
# remove the ID column
train <- read.csv('~/Documents/Github/ERGProject/data/train.csv')[,-1]
test <- read.csv('~/Documents/Github/ERGProject/data/test.csv')[,-1]
# Fix the NA problems
train <- preprocess.fixNA(train)
# Check the frequency of NA in each column
colSums(is.na(train))
y <- train$SalePrice
train <- train[,-ncol(train)]
train.2 <- preprocess.nzv(train,show = 10)
num.feature <- preprocess.getNum(train.2, range= 1:ncol(train.2))
cat.feature <- preprocess.getCat(train.2, range= 1:ncol(train.2))
trainNum <- preprocess.corNum2Num(train.2,num.feature)
trainCat <- preprocess.corCat2Cat(train.2,cat.feature)
remotes::install_github("privefl/bigutilsr")
library(bigutilsr)
library()
train.3 <- merge(trainNum,trainCat)
pca <- prcomp(train.3, scale = T)
knitr::opts_chunk$set(echo = TRUE)
source('~/Documents/Github/ERGProject/preprocessing/preprocess.R')
# Import preparatoins:
library(mice)
library(caret)
library(RANN)
# remove the ID column
train <- read.csv('~/Documents/Github/ERGProject/data/train.csv')[,-1]
test <- read.csv('~/Documents/Github/ERGProject/data/test.csv')[,-1]
# Fix the NA problems
train <- preprocess.fixNA(train)
# Check the frequency of NA in each column
colSums(is.na(train))
y <- train$SalePrice
train <- train[,-ncol(train)]
train.2 <- preprocess.nzv(train,show = 10)
num.feature <- preprocess.getNum(train.2, range= 1:ncol(train.2))
cat.feature <- preprocess.getCat(train.2, range= 1:ncol(train.2))
trainNum <- preprocess.corNum2Num(train.2,num.feature)
trainCat <- preprocess.corCat2Cat(train.2,cat.feature)
train.3 <- merge(trainNum,trainCat)
train.4 <- preprocess.corNum2Cat(train.3,
numIndex = preprocess.getNum(train.3, range = 1:ncol(train.3)),
catIndex = preprocess.getCat(train.3,range = 1:ncol(train.3)))
train.3 <- merge(trainNum,trainCat)
pca <- prcomp(train.3, scale = T)
View(train.3)
source('~/Documents/Github/ERGProject/preprocessing/preprocess.R')
# Import preparatoins:
library(mice)
library(caret)
library(RANN)
# remove the ID column
train <- read.csv('~/Documents/Github/ERGProject/data/train.csv')[,-1]
test <- read.csv('~/Documents/Github/ERGProject/data/test.csv')[,-1]
# Fix the NA problems
train <- preprocess.fixNA(train)
# Check the frequency of NA in each column
colSums(is.na(train))
y <- train$SalePrice
train <- train[,-ncol(train)]
train.2 <- preprocess.nzv(train,show = 10)
num.feature <- preprocess.getNum(train.2, range= 1:ncol(train.2))
cat.feature <- preprocess.getCat(train.2, range= 1:ncol(train.2))
trainNum <- preprocess.corNum2Num(train.2,num.feature)
trainCat <- preprocess.corCat2Cat(train.2,cat.feature)
train.3 <- merge(trainNum,trainCat)
train.3 <- cbind(trainNum,trainCat)
pca <- prcomp(train.3, scale = T)
pca <- prcomp(trainNum, scale = T)
library(ggplot2)
theme_set(bigstatsr::theme_bigstatsr(0.8))
remotes::install_github("privefl/bigutilsr")
library(bigutilsr)
remotes::install_github("privefl/bigutilsr", force = TRUE)
pca <- prcomp(trainNum, scale = T)
library(ggplot2)
#theme_set(bigstatsr::theme_bigstatsr(0.8))
qplot(U[, 1], U[, 2]) + coord_equal()
pca <- prcomp(trainNum, scale = T)
U <- pca$x
library(ggplot2)
#theme_set(bigstatsr::theme_bigstatsr(0.8))
qplot(U[, 1], U[, 2]) + coord_equal()
apply(U, 2, function(x) which( abs(x - mean(x)) > (6 * sd(x)) ))
## integer(0)
library(magrittr)
apply(U, 2, function(x) which( abs(x - mean(x)) > (6 * sd(x)) ))
## integer(0)
library(magrittr)
apply(U, 2, function(x) which( abs(x - mean(x)) > (6 * sd(x)) )) %>%
Reduce(union, .)
## integer(0)
ind.out <- apply(U3, 2, function(x) which( (abs(x - median(x)) / mad(x)) > 6 )) %>%
Reduce(union, .) %>%
print()
ind.out <- apply(U, 2, function(x) which( (abs(x - median(x)) / mad(x)) > 6 )) %>%
Reduce(union, .) %>%
print()
dist <- apply(U, 2, function(x) abs(x - median(x)) / mad(x)) %>%
apply(1, max)
qplot(U[, 1], U[, 3], color = dist, size = I(3)) + coord_equal() +
scale_color_viridis_c(trans = "log", breaks = c(1, 3, 6))
?mad()
dist <- apply(U, 2, function(x) abs(x - median(x)) / mad(x)) %>%
apply(1, max)
qplot(U[, 1], U[, 2], color = dist, size = I(3)) + coord_equal() +
scale_color_viridis_c(trans = "log", breaks = c(1, 3, 6))
# a simple way:
library(magrittr)
a <- apply(U, 2, function(x) which( abs(x - mean(x)) > (6 * sd(x)) )) %>%
Reduce(union, .)
## 1299  314  336  250  935  154  323  471  543  765  925   52   89  171  186  198  199  264  268  407
## 636  730  884 1010 1032 1174 1441
a
trainNum2 <- trainNum[-a,]
pca2 <- prcomp(trainNum2, scale = T)
U2 <- pca2$x
dist <- apply(U2, 2, function(x) abs(x - median(x)) / mad(x)) %>%
apply(1, max)
qplot(U2[, 1], U2[, 2], color = dist, size = I(3)) + coord_equal() +
scale_color_viridis_c(trans = "log", breaks = c(1, 3, 6))
ind.out <- apply(U2, 2, function(x) which( (abs(x - median(x)) / mad(x)) > 6 )) %>%
Reduce(union, .) %>%
print()
ind.out <- apply(U, 2, function(x) which( (abs(x - median(x)) / mad(x)) > 6 )) %>%
Reduce(union, .) %>%
print()
ind.out
trainNum2 <- trainNum[-ind.out,]
pca2 <- prcomp(trainNum2, scale = T)
U2 <- pca2$x
dist <- apply(U2, 2, function(x) abs(x - median(x)) / mad(x)) %>%
apply(1, max)
qplot(U2[, 1], U2[, 2], color = dist, size = I(3)) + coord_equal() +
scale_color_viridis_c(trans = "log", breaks = c(1, 3, 6))
ind.out2 <- apply(U2, 2, function(x) which( (abs(x - median(x)) / mad(x)) > 6 )) %>%
Reduce(union, .) %>%
print()
ind.out2 %in% ind.out
install.packages('FAMD')
remotes::install_github("privefl/bigutilsr", force = TRUE)
install.packages('bigutilsr')
install.packages("bigutilsr")
knitr::opts_chunk$set(echo = TRUE)
source('~/Documents/Github/ERGProject/preprocessing/preprocess.R')
# Import preparatoins:
library(mice)
library(caret)
library(RANN)
# remove the ID column
train <- read.csv('~/Documents/Github/ERGProject/data/train.csv')[,-1]
test <- read.csv('~/Documents/Github/ERGProject/data/test.csv')[,-1]
# Fix the NA problems
train <- preprocess.fixNA(train)
# Check the frequency of NA in each column
colSums(is.na(train))
y <- train$SalePrice
train <- train[,-ncol(train)]
train.2 <- preprocess.nzv(train,show = 10)
num.feature <- preprocess.getNum(train.2, range= 1:ncol(train.2))
cat.feature <- preprocess.getCat(train.2, range= 1:ncol(train.2))
trainNum <- preprocess.corNum2Num(train.2,num.feature)
trainCat <- preprocess.corCat2Cat(train.2,cat.feature)
remotes::install_github("privefl/bigutilsr", force = TRUE)
train.3 <- cbind(trainNum,trainCat)
pca <- prcomp(trainNum, scale = T)
U <- pca$x
library(ggplot2)
#theme_set(bigstatsr::theme_bigstatsr(0.8))
qplot(U[, 1], U[, 2]) + coord_equal()
