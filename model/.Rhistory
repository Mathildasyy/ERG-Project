)
train[,selected]
library(leaps)
library(ranger)
library(e1071)
library(glmnet)
library(boot)
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
regfit.forward = regsubsets(Y~.,data = train, nvmax = 47, method = "forward")
reg.summary <- summary(regfit.forward)
min = which.min(reg.summary$cp)
name <- names(coef(regfit.forward, min))[-1]
selected <- names(train) %in% name
# 1. Forward selection
model.fwd <- function(x){
matrix <- cbind(rep(1,nrow(x)),x[,selected])
pred <- matrix%*%coef(regfit.forward, min)
return(pred)
}
# 2. Principal Component Regression
set.seed(1)
cv_model_pcr <- train(
Y ~ .,
data = train,
method = "pcr",
trControl = fitControl,
tuneLength = 100
)
# 3. Boosting Linear Model
set.seed(1)
BstlmFit <- train(Y~., data = train,
method = 'BstLm',
trControl = fitControl)
# 4. Lasso Regression
set.seed(1)
L2Fit <- train(Y ~ ., data = train,
method = "glmnet",
trControl = fitControl)
# 5. Random Forest Regression
set.seed(1)
tuned_randomforest_model <- ranger(
formula         = Y ~ .,
data            = train,
num.trees       = 1000,
mtry            = 20,
min.node.size   = 3,
sample.fraction = .8,
importance      = 'impurity'
)
RMSLE <- function(data){
pred_pcr <- cv_model_pcr %>% predict(data) # 0.1279845
pred_fwd <- as.numeric(model.fwd(data))  # 0.1191135
pred_bstlm <- predict(BstlmFit,data) # 0.1366453
pre_L2Fit <- as.numeric(predict(L2Fit,data)) # 0.1277953
pred_ranger <- predict(tuned_randomforest_model, data)$predictions # 0.1376053
#pred_svr <- as.numeric(predict(obj_1$best.model, data)) # 0.13454717
prediction <- cbind(pred_fwd, pred_bstlm, pre_L2Fit, pred_ranger, pred_pcr)
coefficient <- c(0.4,0.0025,0.1,0.25,0.22)
rmse <- function(x){
pred <-  prediction %*% x
return(sqrt(mean((pred - data$Y)^2)))
}
coef <- optim(coefficient, rmse)$par
pred <-  prediction %*% coef
print(sqrt(mean((pred - data$Y)^2)))
}
RMSLE(train)
matrix()
matrix
matrix <- cbind(rep(1,nrow(x)),x[,selected])
matrix <- cbind(rep(1,nrow(train)),train[,selected])
matrx
matrix
dim(matrix)
dim(coef(regfit.forward, min))
coef(regfit.forward, min)
class(coef(regfit.forward, min))
length(coef(regfit.forward, min))
names(coef(regfit.forward, min))
names(matrix)
selected <- names(train) %in% name
selected
data <- read.csv('~/Documents/Github/ERG-Project/data/train_full2.csv')[,-1]
set.seed(205)
train.index <- sample(1:nrow(data), 0.7*nrow(data))
train <- data[train.index, ]
test.index <- (-train.index)
test <- data[test.index,]
library(leaps)
library(ranger)
library(e1071)
library(glmnet)
library(boot)
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
regfit.forward = regsubsets(Y~.,data = train, nvmax = 47, method = "forward")
reg.summary <- summary(regfit.forward)
min = which.min(reg.summary$cp)
coef <- coef(regfit.forward, min)
# 1. Forward selection
model.fwd <- function(x){
matrix <- model.matrix(Y~., data =x)[,names(coef)]
pred <- matrix%*%coef
return(pred)
}
# 2. Principal Component Regression
set.seed(1)
cv_model_pcr <- train(
Y ~ .,
data = train,
method = "pcr",
trControl = fitControl,
tuneLength = 100
)
# 3. Boosting Linear Model
set.seed(1)
BstlmFit <- train(Y~., data = train,
method = 'BstLm',
trControl = fitControl)
# 4. Lasso Regression
set.seed(1)
L2Fit <- train(Y ~ ., data = train,
method = "glmnet",
trControl = fitControl)
# 5. Random Forest Regression
set.seed(1)
tuned_randomforest_model <- ranger(
formula         = Y ~ .,
data            = train,
num.trees       = 1000,
mtry            = 20,
min.node.size   = 3,
sample.fraction = .8,
importance      = 'impurity'
)
# 6. SVR
# for (i in 1:length(train)){
#   if (class(train[,i]) == "character"){
#     train[,i] = as.factor(train[,i])
#   }
# }
# set.seed(1)
# tc <- tune.control(cross = 10)
# obj_1 <- tune(svm, Y~., kernal = "linear", data = train, ranges = list(gamma = 2^(-2:2), cost = 2^(2:4)), tunecontrol = tc)
RMSLE <- function(data){
pred_pcr <- cv_model_pcr %>% predict(data) # 0.1279845
pred_fwd <- as.numeric(model.fwd(data2))  # 0.1191135
pred_bstlm <- predict(BstlmFit,data) # 0.1366453
pre_L2Fit <- as.numeric(predict(L2Fit,data)) # 0.1277953
pred_ranger <- predict(tuned_randomforest_model, data)$predictions # 0.1376053
#pred_svr <- as.numeric(predict(obj_1$best.model, data)) # 0.13454717
prediction <- cbind(pred_fwd, pred_bstlm, pre_L2Fit, pred_ranger, pred_pcr)
coefficient <- c(0.4,0.0025,0.1,0.25,0.22)
rmse <- function(x){
pred <-  prediction %*% x
return(sqrt(mean((pred - data$Y)^2)))
}
coef <- optim(coefficient, rmse)$par
pred <-  prediction %*% coef
print(sqrt(mean((pred - data$Y)^2)))
}
RMSLE(train)
RMSLE <- function(data){
pred_pcr <- cv_model_pcr %>% predict(data) # 0.1279845
pred_fwd <- as.numeric(model.fwd(data2))  # 0.1191135
pred_bstlm <- predict(BstlmFit,data) # 0.1366453
pre_L2Fit <- as.numeric(predict(L2Fit,data)) # 0.1277953
pred_ranger <- predict(tuned_randomforest_model, data)$predictions # 0.1376053
#pred_svr <- as.numeric(predict(obj_1$best.model, data)) # 0.13454717
prediction <- cbind(pred_fwd, pred_bstlm, pre_L2Fit, pred_ranger, pred_pcr)
coefficient <- c(0.4,0.0025,0.1,0.25,0.22)
rmse <- function(x){
pred <-  prediction %*% x
return(sqrt(mean((pred - data$Y)^2)))
}
coef <- optim(coefficient, rmse)$par
pred <-  prediction %*% coef
print(sqrt(mean((pred - data$Y)^2)))
}
RMSLE <- function(data){
pred_pcr <- cv_model_pcr %>% predict(data) # 0.1279845
pred_fwd <- as.numeric(model.fwd(data))  # 0.1191135
pred_bstlm <- predict(BstlmFit,data) # 0.1366453
pre_L2Fit <- as.numeric(predict(L2Fit,data)) # 0.1277953
pred_ranger <- predict(tuned_randomforest_model, data)$predictions # 0.1376053
#pred_svr <- as.numeric(predict(obj_1$best.model, data)) # 0.13454717
prediction <- cbind(pred_fwd, pred_bstlm, pre_L2Fit, pred_ranger, pred_pcr)
coefficient <- c(0.4,0.0025,0.1,0.25,0.22)
rmse <- function(x){
pred <-  prediction %*% x
return(sqrt(mean((pred - data$Y)^2)))
}
coef <- optim(coefficient, rmse)$par
pred <-  prediction %*% coef
print(sqrt(mean((pred - data$Y)^2)))
}
RMSLE(train)
RMSLE(test)
data <- read.csv('~/Documents/Github/ERG-Project/data/train_full2.csv')[,-1]
set.seed(2050)
train.index <- sample(1:nrow(data), 0.7*nrow(data))
train <- data[train.index, ]
test.index <- (-train.index)
test <- data[test.index,]
RMSLE(train)
RMSLE(train)
data <- read.csv('~/Documents/Github/ERG-Project/data/train_full2.csv')[,-1]
set.seed(20)
train.index <- sample(1:nrow(data), 0.7*nrow(data))
train <- data[train.index, ]
test.index <- (-train.index)
test <- data[test.index,]
RMSLE(train)
RMSLE(test)
RMSLE(data)
data <- read.csv('~/Documents/Github/ERG-Project/data/train_full2.csv')[,-1]
# set.seed(2050)
# train.index <- sample(1:nrow(data), 0.7*nrow(data))
# train <- data[train.index, ]
# test.index <- (-train.index)
# test <- data[test.index,]
RMSLE(data)
data <- read.csv('~/Documents/Github/ERG-Project/data/train_full2.csv')[,-1]
set.seed(2050)
train.index <- sample(1:nrow(data), 0.7*nrow(data))
train <- data[train.index, ]
test.index <- (-train.index)
test <- data[test.index,]
library(leaps)
library(ranger)
library(e1071)
library(glmnet)
library(boot)
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
regfit.forward = regsubsets(Y~.,data = train, nvmax = 47, method = "forward")
reg.summary <- summary(regfit.forward)
min = which.min(reg.summary$cp)
coef <- coef(regfit.forward, min)
# 1. Forward selection
model.fwd <- function(x){
matrix <- model.matrix(Y~., data =x)[,names(coef)]
pred <- matrix%*%coef
return(pred)
}
# 2. Principal Component Regression
set.seed(1)
cv_model_pcr <- train(
Y ~ .,
data = train,
method = "pcr",
trControl = fitControl,
tuneLength = 100
)
# 3. Boosting Linear Model
set.seed(1)
BstlmFit <- train(Y~., data = train,
method = 'BstLm',
trControl = fitControl)
# 4. Lasso Regression
set.seed(1)
L2Fit <- train(Y ~ ., data = train,
method = "glmnet",
trControl = fitControl)
# 5. Random Forest Regression
set.seed(1)
tuned_randomforest_model <- ranger(
formula         = Y ~ .,
data            = train,
num.trees       = 1000,
mtry            = 20,
min.node.size   = 3,
sample.fraction = .8,
importance      = 'impurity'
)
RMSLE <- function(data){
pred_pcr <- cv_model_pcr %>% predict(data) # 0.1279845
pred_fwd <- as.numeric(model.fwd(data))  # 0.1191135
pred_bstlm <- predict(BstlmFit,data) # 0.1366453
pre_L2Fit <- as.numeric(predict(L2Fit,data)) # 0.1277953
pred_ranger <- predict(tuned_randomforest_model, data)$predictions # 0.1376053
#pred_svr <- as.numeric(predict(obj_1$best.model, data)) # 0.13454717
prediction <- cbind(pred_fwd, pred_bstlm, pre_L2Fit, pred_ranger, pred_pcr)
coefficient <- c(0.4,0.0025,0.1,0.25,0.22)
rmse <- function(x){
pred <-  prediction %*% x
return(sqrt(mean((pred - data$Y)^2)))
}
coef <- optim(coefficient, rmse)$par
pred <-  prediction %*% coef
print(sqrt(mean((pred - data$Y)^2)))
}
RMSLE(train)
RMSLE(test)
data <- read.csv('~/Documents/Github/ERG-Project/data/train_full2.csv')[,-1]
set.seed(205)
train.index <- sample(1:nrow(data), 0.7*nrow(data))
train <- data[train.index, ]
test.index <- (-train.index)
test <- data[test.index,]
library(leaps)
library(ranger)
library(e1071)
library(glmnet)
library(boot)
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
regfit.forward = regsubsets(Y~.,data = train, nvmax = 47, method = "forward")
reg.summary <- summary(regfit.forward)
min = which.min(reg.summary$cp)
coef <- coef(regfit.forward, min)
# 1. Forward selection
model.fwd <- function(x){
matrix <- model.matrix(Y~., data =x)[,names(coef)]
pred <- matrix%*%coef
return(pred)
}
# 2. Principal Component Regression
set.seed(1)
cv_model_pcr <- train(
Y ~ .,
data = train,
method = "pcr",
trControl = fitControl,
tuneLength = 100
)
# 3. Boosting Linear Model
set.seed(1)
BstlmFit <- train(Y~., data = train,
method = 'BstLm',
trControl = fitControl)
# 4. Lasso Regression
set.seed(1)
L2Fit <- train(Y ~ ., data = train,
method = "glmnet",
trControl = fitControl)
# 5. Random Forest Regression
set.seed(1)
tuned_randomforest_model <- ranger(
formula         = Y ~ .,
data            = train,
num.trees       = 1000,
mtry            = 20,
min.node.size   = 3,
sample.fraction = .8,
importance      = 'impurity'
)
RMSLE <- function(data){
pred_pcr <- cv_model_pcr %>% predict(data) # 0.1279845
pred_fwd <- as.numeric(model.fwd(data))  # 0.1191135
pred_bstlm <- predict(BstlmFit,data) # 0.1366453
pre_L2Fit <- as.numeric(predict(L2Fit,data)) # 0.1277953
pred_ranger <- predict(tuned_randomforest_model, data)$predictions # 0.1376053
#pred_svr <- as.numeric(predict(obj_1$best.model, data)) # 0.13454717
prediction <- cbind(pred_fwd, pred_bstlm, pre_L2Fit, pred_ranger, pred_pcr)
coefficient <- c(0.4,0.0025,0.1,0.25,0.22)
rmse <- function(x){
pred <-  prediction %*% x
return(sqrt(mean((pred - data$Y)^2)))
}
coef <- optim(coefficient, rmse)$par
pred <-  prediction %*% coef
print(sqrt(mean((pred - data$Y)^2)))
}
RMSLE(train)
RMSLE(test)
View(test)
View(train)
View(data)
knitr::opts_chunk$set(echo = TRUE)
# 我们先将写好的预处理包导入
source('~/Documents/Github/ERG-Project/preprocessing/preprocess.R')
# 再导入训练数据集
# remove the ID column 除去序列号
train <- read.csv('~/Documents/Github/ERG-Project/data/train.csv')[,-1]
# 剥离“房价”因变量
train.y <- train[,ncol(train)]
# train里保留了所有特征
train <- train[,-ncol(train)]
# preprocess.fixNA的输入：目标特征集；
#                   输出：依照不同分类填补后的特征集
train <- preprocess.feaEngineer(train)
train <- preprocess.fixNA(train)  # Fix the NA problems
#Check the frequency of NA in each column
#colSums(is.na(train)) 经查看，已无缺失值
# preprocess.nzv的输入：目标特征集+显示特征下数据方差接近0的10组特征名称（若不输入，显示全部）
#                 输出：除去数据方差接近0的特征后的特征集
train.2 <- preprocess.nzv(train,show = 10)
# preprocess.getNum的输入：目标特征集
#                    输出：其中是数值的特征序号
num.feature <- preprocess.getNum(train.2)
# preprocess.getCat的输入：目标特征集
#                    输出：其中是类别的特征序号
cat.feature <- preprocess.getCat(train.2)
# preprocess.corNum2Num的输入：目标特征集+其中数值的特征序号（从上获得）
#                        输出：除去高相关性特征后的特征集 (方法：Spearman Correlation)
trainNum <- preprocess.corNum2Num(train.2,num.feature)
# preprocess.corCcat2Cat的输入：目标特征集+其中类别的特征序号（从上获得）
#                         输出：除去5个高相关性特征后的特征集(方法：Cramer's V)
trainCat <- preprocess.corCat2Cat(train.2,cat.feature)
# 通过PCA降维分析甄别异常值
# preprocess.PCA的输入：目标特征集（数值）
#                 输出：被检测为异常值的序号(方法：MAD, Mean Absolute Deviation)
outliers <- preprocess.PCA(trainNum) # Remove and store outliers index
# fix the skewness of the Sale Price
# 平滑处理预测值 log(1+x)
Y <- log1p(train.y[-outliers])
# combine the features and the response
# the full train dataset
# 最终我们得到完整处理完的数据集train.full， 包含以下三部分：
# 1.数值特征，2.类别特征，3.预测值）
train.full <- cbind(trainNum[-outliers,],trainCat[-outliers,], Y)
# 为了方便之后利用，我们将数据导出到data文件夹里，文件命名为“train.full“
write.csv(train.full,'~/Documents/Github/ERG-Project/data/train_full.csv')
# 大家可以按需进行数据预处理~
data <- read.csv('~/Documents/Github/ERG-Project/data/train_full.csv')[,-1]
set.seed(205)
train.index <- sample(1:nrow(data), 0.7*nrow(data))
train <- data[train.index, ]
test.index <- (-train.index)
test <- data[test.index,]
library(leaps)
library(ranger)
library(e1071)
library(glmnet)
library(boot)
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
regfit.forward = regsubsets(Y~.,data = train, nvmax = 47, method = "forward")
reg.summary <- summary(regfit.forward)
min = which.min(reg.summary$cp)
coef <- coef(regfit.forward, min)
# 1. Forward selection
model.fwd <- function(x){
matrix <- model.matrix(Y~., data =x)[,names(coef)]
pred <- matrix%*%coef
return(pred)
}
# 2. Principal Component Regression
set.seed(1)
cv_model_pcr <- train(
Y ~ .,
data = train,
method = "pcr",
trControl = fitControl,
tuneLength = 100
)
# 3. Boosting Linear Model
set.seed(1)
BstlmFit <- train(Y~., data = train,
method = 'BstLm',
trControl = fitControl)
# 4. Lasso Regression
set.seed(1)
L2Fit <- train(Y ~ ., data = train,
method = "glmnet",
trControl = fitControl)
# 5. Random Forest Regression
set.seed(1)
tuned_randomforest_model <- ranger(
formula         = Y ~ .,
data            = train,
num.trees       = 1000,
mtry            = 20,
min.node.size   = 3,
sample.fraction = .8,
importance      = 'impurity'
)
# 6. SVR
# for (i in 1:length(train)){
#   if (class(train[,i]) == "character"){
#     train[,i] = as.factor(train[,i])
#   }
# }
# set.seed(1)
# tc <- tune.control(cross = 10)
# obj_1 <- tune(svm, Y~., kernal = "linear", data = train, ranges = list(gamma = 2^(-2:2), cost = 2^(2:4)), tunecontrol = tc)
RMSLE <- function(data){
pred_pcr <- cv_model_pcr %>% predict(data) # 0.1279845
pred_fwd <- as.numeric(model.fwd(data))  # 0.1191135
pred_bstlm <- predict(BstlmFit,data) # 0.1366453
pre_L2Fit <- as.numeric(predict(L2Fit,data)) # 0.1277953
pred_ranger <- predict(tuned_randomforest_model, data)$predictions # 0.1376053
#pred_svr <- as.numeric(predict(obj_1$best.model, data)) # 0.13454717
prediction <- cbind(pred_fwd, pred_bstlm, pre_L2Fit, pred_ranger, pred_pcr)
coefficient <- c(0.4,0.0025,0.1,0.25,0.22)
rmse <- function(x){
pred <-  prediction %*% x
return(sqrt(mean((pred - data$Y)^2)))
}
coef <- optim(coefficient, rmse)$par
pred <-  prediction %*% coef
print(sqrt(mean((pred - data$Y)^2)))
}
RMSLE(train)
RMSLE(test)
